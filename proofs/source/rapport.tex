\input{./proofs/source/preambule.tex}
\addbibresource{./proofs/source/rapport.bib}
\usepackage{tkz-base}
\usepackage{algorithm}
\usepackage{algorithmic}
\setlength\parindent{0pt}

% \usepackage{graphicx,txfonts}




\title{Rapport de stage:\\
Arbitrages statistiques dans l'apprentissage automatique confidentiel.
}           
\author{{\sc Alexi Canesse}, L3 informatique fondamentale,\\École Normale Supérieure de Lyon\\
Sous la supervision d'{\sc Aurélien Garivier}, Professeur,\\ UMPA et École Normale Supérieure de Lyon}
\date{\today}          

\sloppy                  

\pgfplotsset{compat=1.16}

\begin{document}



\setmathfont{Latin Modern Math}
\setmathfont[range={\mathscr,\mathbfscr}]{XITS Math}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}
TODO

\subsection{Présentation du problème}

TODO

\subsection{Définitions}

La \textit{differential privacy} \cite{10.1007/11681878_14} quantifie la perte de confidentialité subit par un individue en étant dans une base de donné. \\

\definition{Bases de donnée voisines}

On dit que deux bases de données \(x\) et \(y\) sont voisines et on note \(||x - y||_1 \leq 1\) si elles diffèrent sur au plus une entrée \textit{ie} la distance de {\sc Hamming} qui les sépare et majorée par 1.\\

\definition{Differential privacy}

On dit qu'un mécanisme aléatoire \(\mathcal M :\mathcal X^{(\N)} \to \mathcal T\) est \textbf{\((\varepsilon, \delta)\)-\textit{differentially private}} si pour tout \(\mathcal S \subset \mathcal T \) mesurable, 
\[
    \forall x,y \in \mathcal X^{(\N)} \quad ||x - y|| \leq 1 \quad \Rightarrow \quad \mathbb P(\mathcal M(x) \in \mathcal S) \leq \exp(\varepsilon)  \mathbb P(\mathcal M(y) \in \mathcal S) + \delta
\] 
De plus, si \(\delta = 0\), on dit que \(\mathcal M\) est \textbf{\(\varepsilon\)-\textit{differentially private}}.\\


\subsection{Contribution}

TODO

\section{Méthode des histogrammes}

\subsection{AboveThreshold}

Répondre à de nombreuses requête est coûteux en confidentialité. Utiliser à algorithme naïf tel que le mécanisme de {\sc Laplace} \cite{10.1007/11681878_14} ne permet pas de répondre à de nombreuses requêtes avec une bonne précision tout en préservant un bon niveau de confidentialité (\(\varepsilon\) doit être petit). Dans certains cas nous ne sommes néanmoins pas intéressé par les réponses numériques, mais uniquement intéressé par le fait qu'une réponse dépasse ou non un seuil définit. Nous allons voir que \mintinline{cpp}{AboveThreshold} \cite{dwork2014the} permet cela tout en ne payant que pour les requêtes qui dépassent le seuil.

\label{AboveThreshold}
\begin{code}
    AboveThreshold(database, queries, threshold, epsilon){
        Assert("les requêtes sont toutes de sensibilité 1");
        result = 0;
        noisyThreshold = threshold + Lap(2/epsilon);
        for(querie in queries){
            nu = Lap(4/epsilon);
            if(querie(D) + nu > noisyThreshold)
                return result;
            else
                ++result;
        }
        return -1;
    }
\end{code}

L'algorithme venant d'être décrit renvoie l'indice de la première requête à dépasser le seuil si une telle requête existe. C'est une version adaptée de l'algorithme initialement décrit par {\sc Dwork } et {\sc Roth} dans \cite[page 57]{dwork2014the}. Icelui a du sens d'un point de vue informatique mais rend le formalisme mathématiques compliqué (les auteurs eux-même tombent dans ce travers) et nous n'utiliseront pas les légers avantages de leur version.\\
 
\theoreme{}\\
Pour tout ensemble de requêtes \(Q \in \left( \mathcal X^{(\N)} \to  \mathcal T \right)^{\N}\) de sensibilité \(1\), tout seuil \(T \in \R\), tout \(\varepsilon > 0\), \(M : x \in \mathcal X^{(\N)} \mapsto \) \mintinline{cpp}{AboveThreshold(x, Q, T, epsilon)} est \(\varepsilon\)-\textit{differentially private}.\\

\textit{\textbf{Remarque:} La démonstration est une réécriture de celle du livre de référence \cite[page57]{dwork2014the}. Une réécriture était nécessaire car cette démonstration présente de nombreux points limites en terme de rigueur mathématiques et de detail pas suffisant sur certains points non triviaux.}\\ 

\textit{Démonstration}:\\
Soit \(D, D' \in \mathcal X^{(\N)}\) tels que \(||D - D'|| \leq 1\), \(\{f_i\}_i = Q \in \left( \mathcal X^{(\N)} \to  \mathcal T \right)^{\N}\) un ensemble de requêtes de sensibilité \(1\), \(T \in \R\) un seuil, et \(\varepsilon > 0\). On pose alors \(A\) la variable aléatoire \mintinline{cpp}{AboveThreshold(D, Q, T, epsilon)} et \(A'\) la variable aléatoire \mintinline{cpp}{AboveThreshold(D', Q, T, epsilon)}.\\

Soit alors \(k \in \N\). \textit{Montrons que \(\mathbb P (A = k) \leq \mathbb P(A' = k)\)}. En reprenant les notations de l'algorithme [\ref{AboveThreshold}], on fixe les éléments \((\nu_i)_{i {\color{magenta} <} k}\) (qui suivent une loi de {\sc Laplace} de paramètre \(4/\varepsilon\)).\\


On pose alors
\[
    \left\{ 
        \begin{array}[]{rl}
            g_k & = \max_{i {\color{magenta} <} k} \left\{ f_i(D) + \nu_i\right\}\\
            g_k' & = \max_{i {\color{magenta} <} k} \left\{ f_i(D') + \nu_i\right\}\\
        \end{array}
    \right.    
\]

Ces grandeurs représente la valeur plus grande comparée au seuil bruité avant l'indice \(k\) dans le cas de l'execution sur \(D\) et de l'execution sur \(D'\). Les probabilité qui suivent seront prisent sur les deux variables aléatoires non fixées \(\nu_k\) et \(\hat T\) qui est la valeur du seuil bruitée. On pose enfin, pour tout \(i \in \N\),
\[
    \left\{ 
        \begin{array}[]{rl}
            y_i & = f_i(D)\\
            y_i' & = f_i(D')\\
        \end{array}
    \right.    
\]


On note alors que, en notant \(l_2\) la densité de la loi de {\sc Laplace} de paramètre \(2/\varepsilon\) et \(l_4\) celle de paramètre \(4/\varepsilon\),
\begin{align*}
    \mathbb P(A = k) & = \mathbb P(\hat T \in ]g_k, y_k + \nu_k])\\
    & = \int_{\R} \mathbb P(\hat T \in ]g_k, y_k + \nu])l_4(\nu) \dt \nu \\
    & = \int_{\R}\int_{g_k}^{y_k + \nu} l_2(t)l_4(\nu) \dt t \dt \nu 
\end{align*}

On effectue alors un premier changement de variable affine 
\[
    \hat t = t + g_k - g_k'    
\]

On obtient donc 
\begin{align*}
    \mathbb P(A = k) & = \int_{\R}\int_{g_k}^{y_k + \nu} l_2(\hat t - g_k + g_k')l_4(\nu) \dt  t \dt \nu\\
    & = \int_{\R}\int_{g_k'}^{y_k + \nu - g_k + g_k'} l_2(\hat t )l_4(\nu) \dt  t \dt \nu\\
\end{align*}

Il est alors temps de faire un second changement de variable affine
\[
    \hat \nu = \nu + g_k - g_k' + y_k' - y_k   
\]

Ainsi,
\begin{align*}
    \mathbb P(A = k) & = \int_{\R}\int_{g_k'}^{y_k + \nu - g_k + g_k' } l_2(\hat t )l_4(\hat\nu - g_k + g_k' - y_k' + y_k) \dt \hat t \dt \nu\\
    & = \int_{\R}\int_{g_k'}^{y_k + \nu  - g_k +g_k'  + g_k - g_k' +y_k' - y_k} l_2(\hat t )l_4(\hat\nu) \dt  t \dt \nu\\
    & = \int_{\R}\int_{g_k'}^{y_k' + \nu } l_2(\hat t )l_4(\hat\nu) \dt t \dt \nu\\
\end{align*}

Par définition de \(l_2\) et \(l_4\) nous avons donc
\begin{align*}
    \mathbb P(A = k) & =  \int_{\R}\int_{g_k'}^{y_k' + \nu } \exp\left(\dfrac{|\hat t|\varepsilon}{2}\right)\exp\left(\dfrac{|\hat\nu|\varepsilon}{4}\right) \dt t \dt \nu\\
\end{align*}

L'inégalité triangulaire assure alors que 
\begin{align*}
    \mathbb P(A = k) & \leq  \int_{\R}\int_{g_k'}^{y_k' + \nu } \exp\left(\dfrac{|\hat t - t|\varepsilon}{2}\right)\exp\left(\dfrac{|t|\varepsilon}{2}\right)\exp\left(\dfrac{|\hat\nu - \nu|\varepsilon}{4}\right)\exp\left(\dfrac{|\nu|\varepsilon}{4}\right) \dt t \dt \nu\\
    & =  \int_{\R}\int_{g_k'}^{y_k' + \nu } \exp\left(\dfrac{| g_k - g_k' |\varepsilon}{2}\right)\exp\left(\dfrac{|t|\varepsilon}{2}\right)\exp\left(\dfrac{|g_k - g_k' + y_k' - y_k |\varepsilon}{4}\right)\exp\left(\dfrac{|\nu|\varepsilon}{4}\right) \dt  t \dt \nu\\
\end{align*}

Les requêtes étant de sensibilité \(1\), nous avons 
\[
    \left\{ 
        \begin{array}[]{rlc}
            2 & \geq |g_k - g_k'| + |y_k' - y_k | & \geq|g_k - g_k' + y_k' - y_k | \\
            1 & = | g_k - g_k' | \\
        \end{array}
    \right.    
\]

Enfin, la croissance de l'intégrale assure que 
\begin{align*}
    \mathbb P(A = k) & \leq  \int_{\R}\int_{g_k'}^{y_k' + \nu } \exp\left(\dfrac{\varepsilon}{2}\right)\exp\left(\dfrac{|t|\varepsilon}{2}\right)\exp\left(\dfrac{\varepsilon}{2}\right)\exp\left(\dfrac{|\nu|\varepsilon}{4}\right) \dt  t \dt \nu\\
    & = \exp\left(\dfrac{2\varepsilon}{2}\right)  \int_{\R}\int_{g_k'}^{y_k' + \nu }\exp\left(\dfrac{|t|\varepsilon}{2}\right)\exp\left(\dfrac{|\nu|\varepsilon}{4}\right) \dt  t \dt \nu\\
    & = \exp\left(\varepsilon\right) \int_{\R}\int_{g_k'}^{y_k' + \nu }l_2(t)l_4(\nu) \dt  t \dt \nu\\
    & = \exp\left(\varepsilon\right) \int_{\R} \mathbb P(\hat T \in ]g_k', y_k' + \nu]) l_4(\nu) \dt \nu\\
    & = \exp\left(\varepsilon\right) \mathbb P(\hat T \in ]g_k', y_k' + \nu_k]) \\
    & = \exp\left(\varepsilon\right) \mathbb P(A' = k)
\end{align*}




\subsection{La méthode des histogramme}

\subsubsection{Présentation de la méthode des histogrammes}
La méthode des histogramme est une méthode que nous avons proposé durant ce stage. Il s'agit d'une instanciation particulière de \mintinline{cpp}{AboveThreshold} permettant de calculer l'ensemble des déciles (ou n'importe quel quantiles). Une transformation affine permet d'obtenir la réponse finale à partir de la réponse du mécanisme.

\begin{code}
    HistogramMethod(database, epsilon, steps, a, b){
        /* composition theorem */
        epsilon /= 9;

        result = {};
        for(d in {1 ... 9}){ /* which decile */
            T = d*card(database)/10;
            for(i in {1 ... steps}){
                fi = x -> card({element in x | element < i*(b-a)/steps});
                queries.push_back(fi);
            }
            T = d*card(database)/10;
            result.push_back(AboveThreshold(database, queries, T, epsilon)
                                *(b-a)/steps});
        }
        return result;
    }
\end{code}


Les entrée \(a\) et \(b\) donnent une minoration et une majoration de l'ensemble des valeurs d'entrées. L'algorithme découpe alors l'intervalle \([a,b]\) en \mintinline{cpp}{steps} intervalles de même tailles. Pour chaque décile, l'entier renvoyé par \mintinline{cpp}{Abovethreshold} est l'indice de la première valeur à dépasser ce décile.  

\definecolor{mycolor}{RGB}{109,7,26}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
    \draw[mycolor,->] (-7,0) -- (7,0);
    % \draw[white] (-1.425,0) -- (-1.325,0);
    \filldraw[mycolor] (-6,0) circle (2pt) node[below]{$0$};
    \filldraw[mycolor] (-3,0) circle (2pt) node[below]{\(1/4\)};
    \filldraw[mycolor] (0,0) circle (2pt) node[below]{\(1/2\)};
    % \draw[mycolor] (-1.5,-0.3) -- (-1.35, 0.3);
    % \draw[mycolor] (-1.4,-0.3) -- (-1.25, 0.3);
    \filldraw[mycolor] (3,0) circle (2pt) node[below]{3/4};
    \filldraw[mycolor] (6,0) circle (2pt) node[below]{1};
    % \filldraw[red] (2,0) circle (2pt) node[above]{$n$};
    \end{tikzpicture}
    \caption{Le découpage pour \(a = 0\), \(b = 1\), \mintinline{cpp}{steps} = 4}
\end{figure}

\theoreme{}\\
\mintinline{cpp}{HistogramMethod} est \(\varepsilon\)-\textit{differentially private}.\\


\textit{Démonstration:} Les requêtes envoyé par l'algorithme à \mintinline{cpp}{AboveThreshold} sont bien de sensibilité 1. Chacun des neuf appels à cette fonction est donc \(\varepsilon/9\)-\textit{differentially private}. Le théorème de composition assure alors que \mintinline{cpp}{HistogramMethod} est \(\varepsilon\)-\textit{differentially private}.\\

Maintenant que nous avons vu que cet algorithme est bien \textit{differentially private}, nous allons essayer d'évaluer sa précision. Cela ne sera pas évident car la précision de l'algorithme dépend beaucoup du jeu de donné en entré.\\




\lemme{\mintinline{cpp}{AboveThreshold} est \((\alpha, \beta)-accurate\)}\\
Pour tout \(\beta \in ]0,1[\), tout \(x \in \mathcal X^{(\N)}\), tout \(\{f_i\}_i = Q \in \left( \mathcal X^{(\N)} \to  \mathcal T \right)^{\N}\), tout \(\varepsilon > 0\), tout \(T \in \R\),  en posant \(\alpha = 8\left( \log(k) + \log(2/\beta) \right)/\varepsilon\) et \(k = \) \mintinline{cpp}{AboveThreshold(x, Q, T, epsilon)}, on a, en reprenant les notations de l'algorithme,
\[
    \mathbb P \left( \forall i < k \  f_i(x) + \nu_i < T + \alpha \wedge f_k(x) + \nu_k > T - \alpha \right) \geq 1 - \beta
\]

\textit{\textbf{Remarque:} Ce lemme est due à \cite[page 61]{dwork2014the}. Nous reprenons aussi la démonstration ici car la démonstration originale ne nous semble pas assez claire et trop bancale mathématiquement.}\\

\textit{Démonstration:} Reprenons les notations de l'énoncé. Montrons déjà qu'il suffit de démontrer que 
\begin{align}
    \label{accu_lemme}
    \mathbb P\left( \max_{i \leq k} |\nu_i| + |T - \hat T| < \alpha  \right)  \geq 1 - \beta   
\end{align}
où \(\hat T\) est le seuil bruité défini à la ligne 4 de l'algorithme [\ref{AboveThreshold}]. Or, nous avons, en posant pour tout \(i \leq k\), \(y_i = f_i(x)\)
\[
    y_k + \nu_k \geq \hat T \overset{\text{IT}}{\geq} T - |T-\hat T|
\]

\textit{Mutatis mutandis} 
\[
    \forall i < k \quad y_i \leq \hat T + |\nu_i| \leq T + |T - \hat T| + |\nu_i|  
\]

Ainsi,
\[
    \mathbb P \left( \forall i < k \  f_i(x) + \nu_i < T + \alpha \wedge f_k(x) + \nu_k > T - \alpha \right) \geq 1 - \beta
\]

\textit{Démontrons enfin (\ref{accu_lemme})}! La variable aléatoire \(T-\hat T\) suit une loi de {\sc Laplace} de paramètre \(2/\varepsilon\). Ainsi,
\[
    \mathbb P \left( |T - \hat T| \geq \dfrac{\alpha}{2} = \dfrac{\alpha \varepsilon}{4}\dfrac{2}{\varepsilon} \right) = \exp\left( -\dfrac{\varepsilon \alpha}{4} \right) = \exp\left( -2\left( \log k + \log \dfrac{2}{\beta} \right) \right) \leq \exp\left( -2\left(\log \dfrac{2}{\beta} \right) \right) \leq \dfrac{\beta}{2}
\]

De même,
\[
    \mathbb P \left( \max_{i} |\nu_i| \geq \dfrac{\alpha}{2} \right) \leq \sum\limits_{j = 1}^k \mathbb P \left( |\nu_j| \geq \dfrac{\alpha}{2} \right) = k \exp\left( -\dfrac{-\alpha\varepsilon}{8} \right) = k \exp\left( -\log k - \log\dfrac{2}{\beta} \right) = \dfrac{k}{k} \dfrac{\beta}{2}
\]

Enfin, 
\begin{align*}
    \mathbb P\left( \max_{i \leq k} |\nu_i| + |T - \hat T| < \alpha  \right) & \geq \mathbb P\left( \max_{i \leq k} |\nu_i| < \dfrac{\alpha}{2} \ \wedge \ |T - \hat T| < \dfrac{\alpha}{2} \right)\\
    & = 1 - \mathbb P\left( \max_{i \leq k} |\nu_i| \geq \dfrac{\alpha}{2} \ \cup \ |T - \hat T| \geq \dfrac{\alpha}{2} \right)\\
    & \geq 1 - \mathbb P\left( \max_{i \leq k} |\nu_i| \geq \dfrac{\alpha}{2}\right) - \mathbb P \left(|T - \hat T| \geq \dfrac{\alpha}{2} \right)\\
    &\geq 1 - \dfrac{\beta}{2} - \dfrac{\beta}{2}
\end{align*}

Finalement, 
\[
    \mathbb P\left( \max_{i \leq k} |\nu_i| + |T - \hat T| < \alpha  \right)  \geq 1 - \beta
\]

Ce qui démontre bien (\ref{accu_lemme}) et donc le lemme.\\

\subsubsection{Analyse de complexité}

La complexité de \mintinline{cpp}{AboveThreshold} est de l'ordre de la somme des complexité des requêtes sur le jeu de données d'entré. En notant \(n\) la taille de la base de donnée, les requêtes envoyé à \mintinline{cpp}{AboveThreshold} par \mintinline{cpp}{HistogramMethod} sont toute de complexité linéaire en \(n\). La variable \mintinline{cpp}{step} a aussi pour valeur le nombre de requêtes envoyées que l'on nommera \(k\). L'algorithme a alors une complexité en \(\O(nk)\).

\subsubsection{Analyse de précision - le cas de la distribution uniforme}

Nous allons évaluer la précision de l'algorithme à l'aide de l'erreur quadratique moyenne entre la valeur renvoyé par le programme et la valeur attendue. Il y a plusieurs manière de penser ce qu'est la valeur attendue: elle pourrait être la valeur des déciles de l'échantillons d'entré. Néanmoins, elle peut tout aussi bien être l'ensemble des déciles de la loi. En effet, nous cherchons à répondre à des questions de statistique, l'entré peut-être un simple échantillon ``représentatif'';  au quel cas nous sommes principalement intéressé par les réponses statistiques sur l'ensemble de la population et non juste sur notre échantillon.\\

Ces deux choix ont un réel sens. Nous avons d'abord essayé d'évaluer les performances de l'algorithme dans le premier cas. Les calculs était difficiles et menaient à des résultats difficilement exploitables. Nous avons donc choisi de réaliser les calculs sur la seconde option afin de pouvoir mener des calculs légèrement plus simples et ainsi avoir des résultats.\\

Nous allons commencer par démontrer quelques lemmes intermédiaires afin de démontrer les résultats de précision.\\

\lemme{Estimation de l'écard entre les déciles empiriques et ceux de la loi uniforme.}\\
Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1] et soit \(\gamma \in [0,0.1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\). Pour tout \(i \in \inte 1 9\)
\[
    \mathbb P(d_i \in [0.1i - \gamma/2, 0.1i + \gamma/2]) \geq 1 - \eta
\]
Avec 
\begin{align*}
    \eta & = 2 - \sum_{k = 0}^{in/10} \binom{n}{k}(0.1i + \gamma/2)^k (1 - 0.1i - \gamma/2)^{n - k}\\
    & \quad + \sum_{k = 0}^{(10-i)n/10} \binom{n}{k} (1 - 0.1i + \gamma/2)^k (0.1i - \gamma/2)^{n - k}    
\end{align*}




\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Notons \((d_i)_i\) les déciles empiriques de \(X\). Soit \(\gamma \in [0,0.1]\). On note que
\begin{align*}
    \mathbb P(d_i \in [0.1i - \gamma/2, 0.1i + \gamma/2]) & = 1 - \mathbb P(d_i \notin [0.1i - \gamma/2, 0.1i + \gamma/2])\\
    & = 1 - \mathbb P(d_i \leq 0.1i - \gamma/2 \vee d_i \geq 0.1i + \gamma/2])
\end{align*}

On pose alors \(A = \) ``il y a au moins \(in/10\) valeurs plus petites que \(0.1i + \gamma/2\)'' et \(B = \) ``il y a au moins \((10-i)n/10\) valeurs plus grandes que \(0.1i - \gamma/2\)''. On a alors

\begin{align*}
    \mathbb P(d_i \in [0.1i - \gamma/2, 0.1i + \gamma/2]) & = 1 - \mathbb P(A \cup B)\\
    & \geq 1 - \mathbb P(A) - \mathbb P(B)\\
    & = \sum_{k = 0}^{in/10}\binom{n}{k}(0.1i + \gamma/2)^k (1 - 0.1i - \gamma/2)^{n - k} \\
    & \quad \quad + \sum_{k = 0}^{(10-i)n/10}\binom{n}{k} (1 - 0.1i + \gamma/2)^k (0.1i - \gamma/2)^{n - k} - 1
\end{align*}

\lemme{}

Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Soit \(\gamma \in [0,0.1]\), \(i \in \inte 1 9 \) et \(\alpha \in \N\). Il y a au moins \(\alpha\) valeurs de \(X\) dans chacun des intervalles \([0.1i - \gamma, 0.1i-\gamma/2]\) et \([0.1i + \gamma/2, 0.1i+\gamma]\) avec une probabilité au moins \(1 - \beta\) avec 
\[
    \beta = 2\sum_{k = 0}^\alpha \binom{n}{k}\left( \dfrac{\gamma}{2} \right)^k \left( 1 - \dfrac{\gamma}{2} \right)^{n - k}  
\]

\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Soit \(\gamma \in [0,0.1]\), \(i \in \inte 1 9 \) et \(k \in \N\). On note que 
\begin{align*}
    &\ \mathbb P \left( \#\{x \in X | x \in [0.1*i - \gamma, 0.1*i - \gamma/2]\} \geq \alpha \wedge \#\{x \in X | x \in [0.1*i + \gamma/2, 0.1*i + \gamma]\} \geq \alpha \right) \\
    \geq &\ \mathbb P \left( \#\{x \in X | x \in [0.1*i - \gamma, 0.1*i - \gamma/2]\} \geq \alpha\right)\\
    &\quad  + \mathbb P\left( \#\{x \in X | x \in [0.1*i + \gamma/2, 0.1*i + \gamma]\} \geq \alpha \right) - 1 \\
    = &\ 1-2\sum_{k = 0}^\alpha \binom{n}{k}\left( \dfrac{\gamma}{2} \right)^k \left( 1 - \dfrac{\gamma}{2} \right)^{n - k}
\end{align*}

La combinaison des trois lemmes précédents permet d'obtenir un résultat de précision utile sur \mintinline{cpp}{HistogramMethod}.\\


\theoreme{\((\alpha, \beta)\)-précision de \mintinline{cpp}{HistogramMethod} dans le cas uniforme sur \([0,1]\)}

Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Soit \(\gamma \in [0,0.1]\), \(i \in \inte 1 9 \), \(k \in \N\) et \(\beta \in [0,1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\). Posons \(A\) la variable aléatoire \mintinline{cpp}{HistogramMethod(X, epsilon, k, a, b)}.

\[
    \mathbb P\left( A_i \in [0.1i-\gamma, 0.1i + \gamma] \right) \geq 1 - \beta - \eta -\mu    
\]
Avec 
\[
    \left\{ 
        \begin{array}{rl}
            \alpha & = \dfrac{8\left( \log k + \log(2/\beta) \right)}{\varepsilon} \\
            \mu & = 2\sum_{k = 0}^\alpha \binom{n}{k}\left( \dfrac{\gamma}{2} \right)^k \left( 1 - \dfrac{\gamma}{2} \right)^{n - k} \\
            \eta & = 2 - \sum_{k = 0}^{in/10} \binom{n}{k}(0.1i + \gamma/2)^k (1 - 0.1i - \gamma/2)^{n - k}\\
            & \quad + \sum_{k = 0}^{(10-i)n/10} \binom{n}{k} (1 - 0.1i + \gamma/2)^k (0.1i - \gamma/2)^{n - k}  
        \end{array}
    \right.    
\]

\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Soit \(\gamma \in [0,0.1]\), \(i \in \inte 1 9 \), \(k \in \N\) et \(\beta \in [0,1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\). Posons \(A\) la variable aléatoire \mintinline{cpp}{HistogramMethod(X, epsilon, k, a, b)}.\\

On pose 
\[
    \alpha = \dfrac{8\left( \log k + \log(2/\beta) \right)}{\varepsilon}    
\]

Notons alors \(E_\alpha\) l'événement ``Il y a au moins \(\alpha\) valeurs de \(X\) dans chacun des intervalles \([0.1i - \gamma, 0.1i-\gamma/2]\) et \([0.1i + \gamma/2, 0.1i+\gamma]\)'' Et \(E_{A_i}\) l'événement ``moins de \(\alpha\) valeurs de \(X\) séparent \(d_i\) et \(A_i\)''. Nous avons alors 
\begin{align*}
    \mathbb P\left( A_i \in [0.1i-\gamma, 0.1i + \gamma] \right) & \geq \mathbb P \left( E_{A_i} \wedge E_\alpha \wedge d_i \in [0.1i - \gamma/2, 0.1i + \gamma/2]  \right)\\
    & \geq \mathbb P \left( E_{A_i}\right) + \mathbb P \left( E_\alpha\right) + \mathbb P \left( d_i \in [0.1i - \gamma/2, 0.1i + \gamma/2]  \right) - 2\\
\end{align*}

Les trois précédents lemmes assurent que 
\begin{align*}
    \mathbb P\left( A_i \in [0.1i-\gamma, 0.1i + \gamma] \right) & \geq (1 - \beta) + (1 - \mu) + (1 - \eta) - 2\\
    & \geq 1 - \beta - \mu - \eta
\end{align*}

Avec 
\[
    \left\{ 
        \begin{array}{rl}
            \mu & = 2\sum_{k = 0}^\alpha \binom{n}{k}\left( \dfrac{\gamma}{2} \right)^k \left( 1 - \dfrac{\gamma}{2} \right)^{n - k} \\
            \eta & = 2 - \sum_{k = 0}^{in/10} \binom{n}{k}(0.1i + \gamma/2)^k (1 - 0.1i - \gamma/2)^{n - k}\\
            & \quad + \sum_{k = 0}^{(10-i)n/10} \binom{n}{k} (1 - 0.1i + \gamma/2)^k (0.1i - \gamma/2)^{n - k}  
        \end{array}
    \right.    
\]

\subsubsection{Analyse de précision - le cas de la loi normale centrée réduite}

Les lois normales est très utilisées en statistique notamment car elle permettent de modéliser les phénomènes issues de plusieurs événement aléatoires. Le théorème central limite viens jouer un rôle clé dans la prépondérance de l'utilisation de ces lois. Il semble alors crucial d'étudier la précision de notre algorithme dans le cas où les données d'entré suivent une loi normale.\\

Le théorème de précision est très analogue à celui obtenue dans le cas uniforme. Nous ne détaillons pas ici les lemmes intermédiaires et la démonstration car il s'agit formellement de la même chose. Il est néanmoins nécessaire d'introduire quelques objets usuels en plus car la loi normale est plus complexe que la loi uniforme.\\

\definition{Fonction d'erreur}

On appel fonction d'erreur la fonction suivant:
\[
        \erf: \left\{ 
        \begin{array}[]{rcl}
            \C &\to& \C\\
            z &\mapsto& \dfrac{2}{\sqrt \pi} \int_0^z\exp\left( -t^2 \right)\dt t
        \end{array}
        \right.
\]

\lemme{Déciles de \(\mathcal N(0,1)\).}\\
Les déciles de \(\mathcal N(0,1)\), notés \((d^l_i)_i\) sont 
\[
    \forall i \in \inte 1 9 \quad d^l_i = \sqrt{2} \erf^{-1}(2\times 0.1i - 1)
\]

\textit{Démonstration:} Soit \(i \in \inte 1 9 \). On note que 
\begin{align*}
    \dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{d^l_i} \exp\left( \dfrac{-t^2}{2}\right)\dt t &  = \dfrac{\sqrt{2}}{\sqrt{2\pi}}\int_{-\infty}^{\erf^{-1}(2\times 0.1i - 1)} \exp\left( -t^2\right)\dt t \\
    & =\dfrac{1}{2} \dfrac{2}{\sqrt{\pi}}\int_{-\infty}^{\erf^{-1}(2\times 0.1i - 1)} \exp\left( -t^2\right)\dt t \\
    & =\dfrac{1}{2} \erf\left({\erf^{-1}(2\times 0.1i - 1)}\right) + \dfrac{1}{2} \dfrac{2}{\sqrt{\pi}}\int_{-\infty}^0 \exp\left( -t^2 \right)\dt t\\
    & = 0.1i - \dfrac{1}{2} + \dfrac{1}{2}\\
    & = 0.1i
\end{align*}
La démonstration dans le cas d'une loi normale est analogue à celle du cas uniforme. Nous aurons donc des lemmes similaires. Les démonstrations seront néanmoins laissées en appendix [\ref{hmncr}].\\

\lemme{Estimation de l'écard entre les déciles empiriques et ceux de la loi normale centrée réduite.} \label{ecard_deciles_empirique_loi_n01}\\
Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite et soit \(\gamma \in [0, d_i^l]\). Notons \((d_i)_i\) les déciles empiriques de \(X\) et \((d_i^l)_i\) les déciles de la loi normale centrée réduite. Pour tout \(i \in \inte 1 9\)
\[
    \mathbb P(d_i \in [d_i^l - \gamma/2, d_i^l + \gamma/2]) \geq 1 - \eta
\]
Avec 
\begin{align*}
    \eta & = 2 - \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}\\
    & \quad + \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}    
\end{align*}

\lemme{}\label{ecard_deciles_empirique_loi_n02}\\
Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la normale centrée réduite. Soit \(\gamma \in [0,d^l_i]\), \(i \in \inte 1 9 \) et \(k \in \N\). Il y a au moins \(\alpha\) valeurs de \(X\) dans chacun des intervalles \([d^l_i - \gamma, d^l_i-\gamma/2]\) et \([d^l_i + \gamma/2, d^l_i+\gamma]\) avec une probabilité au moins \(1 - \beta\) avec 
\begin{align*}
    \beta & = \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
    & \quad + \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
\end{align*} 

\theoreme{\((\alpha, \beta)\)-précision de \mintinline{cpp}{HistogramMethod} dans le cas de la loi normale centrée réduite}\label{ecard_deciles_empirique_loi_n03}\\
Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite. Soit \(\gamma \in [0,d_i^l]\), \(i \in \inte 1 9 \), \(k \in \N\) et \(\beta \in [0,1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\) et \((d_i^l)_i\) les déciles de la loi normale centrée réduite. Posons \(A\) la variable aléatoire \mintinline{cpp}{HistogramMethod(X, epsilon, k, a, b)}.

\[
    \mathbb P\left( A_i \in [d_i^l-\gamma, d_i^l + \gamma] \right) \geq 1 - \beta - \eta -\mu    
\]
Avec 
\[
    \left\{ 
        \begin{array}{rl}
            \alpha & = \dfrac{8\left( \log k + \log(2/\beta) \right)}{\varepsilon} \\
            \mu & = \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
            & \quad + \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
            \eta & = 2 - \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}\\
            & \quad + \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} 
        \end{array}
    \right.    
\]



\section{Le mécanisme de sensibilité inverse}

\subsection{Présentation du mécanisme}

Le mécanisme de sensibilité inverse est introduit par {\sc Hilal Asi} and {\sc John C. Duchi} dans \textit{Near Instance-Optimality in Differential Privacy} \cite{Asi2020NearII}. Le mécanisme considère l'inverse du nombre de valeurs à modifié dans un ensemble de donnée pour passer à un autre ensemble de donné sur lequel la requête a une autre valeur recherchée. Cela définit alors l'utilité d'une valeur pour instancier le mécanisme exponentiel \cite{mcsherry2007mechanism}.\\

\definition{Longueur}

Soit \(x \in \mathcal X^{(\N)}\), \(f : \mathcal X^{(\N)} \to \mathcal T\) et \(t \in \mathcal T\). La longueur est le nombre minimum de valeurs à modifier dans \(x\) pour obtenir \(x'\) tel que \(f(x') = t\). 
\[
    \len_f(x,t) = \inf_{x' \in \mathcal X^{(\N)}}\left\{|| x - x'||_1 \ |\ f(x') = t \right\}    
\]

\definition{Mécanisme de sensibilité inverse}

Soit \(f : \mathcal X^{(\N)} \to \mathcal T\) et \(\varepsilon \in \R_+\). Pour une mesure \(\mu\) sur \(\mathcal T\), on définit le mécanisme aléatoire \(M(x)\) par sa fonction de densité 
\[
    t \mapsto \dfrac{\exp(-\len_f(x, t)\varepsilon/2)}{\int_\mathcal T \exp(-\len_f(x, s)\varepsilon/2)\dt \mu(s)}    
\] 

Il n'y a qu'en \(f(x)\) que \(\len_f(x,•)\) est nulle. Ainsi le dénominateur pourrait être petit est donné une grande probabilité à des valeurs distantes de \(f(x)\). On \cite{mcsherry2007mechanism} introduit alors une version lisse du mécanisme.\\

\definition{Longueur lisse}

Soit \(x \in \mathcal X^{(\N)}\), \(f : \mathcal X^{(\N)} \to \mathcal T\) et \(\rho \in \R_+\). Si \(\mathcal N\) est une norme sur \(\mathcal T\),
\[
    \len_f^{\rho} : 
    \left\{
        \begin{array}[]{rcl}
            \mathcal T & \to & \N\\
            t & \mapsto & \inf_{s \in \mathcal T, \mathcal N(s,t) \leq \rho}\left\{\len_f(x,s) \right\}  
        \end{array}
    \right.    
\]

\definition{Mécanisme de sensibilité inverse \(\rho\)-lisse}

Soit \(f : \mathcal X^{(\N)} \to \mathcal T\) et \(\rho, \varepsilon \in \R_+\). Pour une mesure \(\mu\) sur \(\mathcal T\), on définit le mécanisme aléatoire \(M_{\text{cont}}(x)\) par sa fonction de densité 
\[
    t \mapsto \dfrac{\exp(-\len_f^\rho(x, t)\varepsilon/2)}{\int_\mathcal T \exp(-\len_f^\rho(x, s)\varepsilon/2)\dt \mu(s)}    
\] 

\theoreme{}\\
Pour tout \(\rho,\varepsilon \in \R_+\), le mécanisme de sensibilité inverse \(\rho\)-lisse est \(\varepsilon\)-\textit{differentially private}. \\

\textit{Démonstration: } Soit \(f : \mathcal X^{(\N)} \to \mathcal T\), \(\rho, \varepsilon \in \R_+\), \(\mu\) une mesure sur \(\mathcal T\), \(\mathcal S \subset \mathcal T\) mesurable et \(x,x' \in \mathcal X^{(\N)}\) voisines. \\

On note que 
\begin{align*}
    \mathbb P\left( M_{\text{cont}}(x) \in \mathcal S \right) & = \int_\mathcal S \dfrac{\exp(-\len_f^\rho(x, t)\varepsilon/2)}{\int_\mathcal T \exp(-\len_f^\rho(x, s)\varepsilon/2)\dt \mu(s)}    \dt \mu(t)\\
    & \leq \int_\mathcal S \dfrac{\exp(-(\len_f^\rho(x', t)-1)\varepsilon/2)}{\int_\mathcal T \exp(-(\len_f^\rho(x', s)+1)\varepsilon/2)\dt \mu(s)}    \dt \mu(t)\\
    & = \dfrac{\exp(\varepsilon/2)}{\exp(-\varepsilon/2)}\int_\mathcal S \dfrac{\exp(-\len_f^\rho(x', t)\varepsilon/2)}{\int_\mathcal T \exp(-\len_f^\rho(x, s)\varepsilon/2)\dt \mu(s)}    \dt \mu(t)\\
    & = \exp(\varepsilon) \mathbb P\left( M_{\text{cont}}(x') \in \mathcal S \right)
\end{align*}


\subsection{Quasi-optimalité du mécanisme de sensibilité inverse}

\definition{Fonctions échantillon-monotone.}

Soit \(f : \mathcal X^{(\N)} \to \R\). On dit que \(f\) est \textbf{échantillon-monotone} si pour tout \(x \in \mathcal X^n\) et \(s,t \in \R\) tels que \(f(x) \leq s \leq t\) ou \(t \leq s \leq f(x)\), 
\[
    \len_f(x,s) \leq \len_f(x,t)
\]






























\newpage

\appendix
\section{HistogramMethod: Analyse de précision - le cas de la loi normale centrée réduite}
\label{hmncr}
\subsection{Démonstration du lemme [\ref{ecard_deciles_empirique_loi_n01}]}

\lemme{Estimation de l'écard entre les déciles empiriques et ceux de la loi normale centrée réduite.}\\
Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite et soit \(\gamma \in [0, d_i^l]\). Notons \((d_i)_i\) les déciles empiriques de \(X\) et \((d_i^l)_i\) les déciles de la loi normale centrée réduite. Pour tout \(i \in \inte 1 9\)
\[
    \mathbb P(d_i \in [d_i^l - \gamma/2, d_i^l + \gamma/2]) \geq 1 - \eta
\]
Avec 
\begin{align*}
    \eta & = 2 - \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}\\
    & \quad + \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}    
\end{align*}

\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi uniforme sur [0,1]. Notons \((d_i)_i\) les déciles empiriques de \(X\). Soit \(\gamma \in [0,d^l_i]\). On note que
\begin{align*}
    \mathbb P(d_i \in [d^l_i - \gamma/2, d^l_i + \gamma+2]) & = 1 - \mathbb P(d_i \notin [d^l_i - \gamma/2, d^l_i + \gamma/2])\\
    & = 1 - \mathbb P(d_i \leq d^l_i - \gamma/2 \vee d_i \geq d^l_i + \gamma/2)
\end{align*}

On pose alors \(A = \) ``il y a au moins \(in/10\) valeurs plus petites que \(d^l_i + \gamma/2\)'' et \(B = \) ``il y a au moins \((10-i)n/10\) valeurs plus grandes que \(d^l_i - \gamma/2\)''. On a alors

\begin{align*}
    &\ \mathbb P(d_i \in [d^l_i - \gamma/2, d^l_i + \gamma/2])\\
    = &\ 1 - \mathbb P(A \cup B)\\
    \geq &\ 1 - \mathbb P(A) - \mathbb P(B)\\
    = &\ \sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\dfrac{1}{\sqrt{2\pi}}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\dfrac{1}{\sqrt{2\pi}}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} \\
    & \quad \quad + \sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\dfrac{1}{\sqrt{2\pi}}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\dfrac{1}{\sqrt{2\pi}}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} 
    - 1\\
    = &\  \dfrac{1}{\sqrt{2\pi}^n} \sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} \\
    & \quad \quad + \dfrac{1}{\sqrt{2\pi}^n}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} 
    - 1
\end{align*}

\subsection{Démonstration du lemme [\ref{ecard_deciles_empirique_loi_n02}]}
\lemme{}

Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la normale centrée réduite. Soit \(\gamma \in [0,d^l_i]\), \(i \in \inte 1 9 \) et \(k \in \N\). Il y a au moins \(\alpha\) valeurs de \(X\) dans chacun des intervalles \([d^l_i - \gamma, d^l_i-\gamma/2]\) et \([d^l_i + \gamma/2, d^l_i+\gamma]\) avec une probabilité au moins \(1 - \beta\) avec 
\begin{align*}
    \beta & = \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
    & \quad + \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
\end{align*}

\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite. Soit \(\gamma \in [0,d^l_i]\), \(i \in \inte 1 9 \) et \(\alpha \in \N\). On note que 
\begin{align*}
    &\ \mathbb P \left( \#\{x \in X | x \in [d^l_i - \gamma, d^l_i - \gamma/2]\} \geq \alpha \wedge \#\{x \in X | x \in [d^l_i + \gamma/2, d^l_i + \gamma]\} \geq \alpha \right) \\
    \geq &\ \mathbb P \left( \#\{x \in X | x \in [d^l_i - \gamma, d^l_i - \gamma/2]\} \geq \alpha\right)\\
    &\quad  + \mathbb P\left( \#\{x \in X | x \in [d^l_i + \gamma/2, d^l_i + \gamma]\} \geq \alpha \right) - 1 \\
    = &\ 1- \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
    & \quad - \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
\end{align*}

\subsection{Démonstration du théorème [\ref{ecard_deciles_empirique_loi_n03}]}

\theoreme{\((\alpha, \beta)\)-précision de \mintinline{cpp}{HistogramMethod} dans le cas de la loi normale centrée réduite}

Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite. Soit \(\gamma \in [0,d_i^l]\), \(i \in \inte 1 9 \), \(k \in \N\) et \(\beta \in [0,1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\) et \((d_i^l)_i\) les déciles de la loi normale centrée réduite. Posons \(A\) la variable aléatoire \mintinline{cpp}{HistogramMethod(X, epsilon, k, a, b)}.

\[
    \mathbb P\left( A_i \in [d_i^l-\gamma, d_i^l + \gamma] \right) \geq 1 - \beta - \eta -\mu    
\]
Avec 
\[
    \left\{ 
        \begin{array}{rl}
            \alpha & = \dfrac{8\left( \log k + \log(2/\beta) \right)}{\varepsilon} \\
            \mu & = \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
            & \quad + \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
            \eta & = 2 - \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}\\
            & \quad + \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} 
        \end{array}
    \right.    
\]

\textit{Démonstration:} Soit \(X\) un ensemble de \(n\) variables aléatoires \((X_i)_i\) indépendantes et suivant toutes la loi normale centrée réduite. Soit \(\gamma \in [0,d_i^l]\), \(i \in \inte 1 9 \), \(k \in \N\) et \(\beta \in [0,1]\). Notons \((d_i)_i\) les déciles empiriques de \(X\) et \((d_i^l)_i\) les déciles de la loi normale centrée réduite. Posons \(A\) la variable aléatoire \mintinline{cpp}{HistogramMethod(X, epsilon, k, a, b)}.\\

On pose 
\[
    \alpha = \dfrac{8\left( \log k + \log(2/\beta) \right)}{\varepsilon}    
\]

Notons alors \(E_\alpha\) l'événement ``Il y a au moins \(\alpha\) valeurs de \(X\) dans chacun des intervalles \([d_i^l - \gamma, d_i^l-\gamma/2]\) et \([d_i^l + \gamma/2, d_i^l+\gamma]\)'' Et \(E_{A_i}\) l'événement ``moins de \(\alpha\) valeurs de \(X\) séparent \(d_i\) et \(A_i\)''. Nous avons alors 
\begin{align*}
    \mathbb P\left( A_i \in [d_i^l-\gamma, d_i^l + \gamma] \right) & \geq \mathbb P \left( E_{A_i} \wedge E_\alpha \wedge d_i \in [d_i^l - \gamma/2, d_i^l + \gamma/2]  \right)\\
    & \geq \mathbb P \left( E_{A_i}\right) + \mathbb P \left( E_\alpha\right) + \mathbb P \left( d_i \in [d_i^l - \gamma/2, d_i^l + \gamma/2]  \right) - 2\\
\end{align*}

Les lemmes précédent assurent alors que 
\begin{align*}
    \mathbb P\left( A_i \in [0.1i-\gamma, 0.1i + \gamma] \right) & \geq (1 - \beta) + (1 - \mu) + (1 - \eta) - 2\\
    & \geq 1 - \beta - \mu - \eta
\end{align*}

Avec 
\[
    \left\{ 
        \begin{array}{rl}
            \mu & = \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i - \gamma}^{d^l_i -\gamma/2}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^{n - k}\\
            & \quad + \sum_{k = 0}^\alpha \binom{n}{k}\left( \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t \right)^k \left( 1 - \int_{d^l_i + \gamma/2}^{d^l_i +\gamma}\dfrac{1}{\sqrt{2\pi}} \exp\left( \dfrac{-t^2}{2} \right)\dt t\right)^{n - k}\\
            \eta & = 2 - \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{in/10}\binom{n}{k}\left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k}\\
            & \quad + \dfrac{1}{(2\pi)^{n/2}}\sum_{k = 0}^{(10-i)n/10}\binom{n}{k}\left( \int_{d^l_i}^{+\infty}\exp\left( -\dfrac{t^2}{2} \right)\dt t \right)^k \left( \int_{-\infty}^{d^l_i}\exp\left( -\dfrac{t^2}{2} \right)\dt t  \right)^{n - k} 
        \end{array}
    \right.    
\]
\newpage
\printbibliography
% \vspace{30px}
% \vspace{15px}


\end{document} 